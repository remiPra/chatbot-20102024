<!DOCTYPE html>
<html>
<head>
    <title>Assistant Vocal IA</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Poppins', sans-serif;
        }
        .gradient-bg {
            background: linear-gradient(120deg, #6366f1, #8b5cf6);
        }
        .glass-effect {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
        }
        .pulse-animation {
            animation: pulse 2s infinite;
        }
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }
    </style>
</head>
{% include 'drawer.html' %} 
<body class="gradient-bg min-h-screen py-12 px-4">
    <div class="max-w-3xl mx-auto">
        <div class="glass-effect rounded-2xl shadow-xl p-8">
            <h1 class="text-3xl font-bold text-center mb-8 text-indigo-700">
                Assistant Vocal IA
            </h1>
            
            <div class="flex justify-center mb-8">
                <button id="recordButton" 
                        class="px-8 py-4 rounded-full font-semibold text-white transition-all duration-300 shadow-lg hover:shadow-xl flex items-center space-x-2"
                        style="background: linear-gradient(45deg, #ef4444, #f43f5e)">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
                    </svg>
                    <span>Commencer l'enregistrement</span>
                </button>
            </div>
            
            <div id="status" class="text-center mb-6 text-indigo-600 font-medium"></div>
            
            <div id="result" class="mt-8 hidden">
                <div class="space-y-6">
                    <div class="bg-white rounded-xl p-6 shadow-md">
                        <h2 class="text-xl font-semibold mb-3 text-gray-800">Votre message</h2>
                        <div id="transcription" class="text-gray-600"></div>
                    </div>
                    
                    <div class="bg-indigo-50 rounded-xl p-6 shadow-md">
                        <h2 class="text-xl font-semibold mb-3 text-indigo-700">Réponse de l'IA</h2>
                        <div id="llm-response" class="text-gray-700"></div>
                    </div>
                </div>
            </div>
            
            <audio id="audioPlayer" class="hidden"></audio>
        </div>
    </div>

    <script>
    let mediaRecorder;
    let audioChunks = [];
    let isRecording = false;
    let silenceTimer = null;
    let audioContext;
    let analyser;
    let dataArray;
    let silenceDetectionInterval;
    const SILENCE_THRESHOLD = 10;
    const SILENCE_DURATION = 1500;

    async function setupAudio() {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        audioContext = new AudioContext();
        analyser = audioContext.createAnalyser();
        const source = audioContext.createMediaStreamSource(stream);
        source.connect(analyser);
        
        analyser.fftSize = 2048;
        dataArray = new Uint8Array(analyser.frequencyBinCount);
        
        mediaRecorder = new MediaRecorder(stream, {
            mimeType: 'audio/webm'
        });
        
        mediaRecorder.ondataavailable = (event) => {
            audioChunks.push(event.data);
        };
        
        mediaRecorder.onstop = async () => {
    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
    const formData = new FormData();
    formData.append('audio', audioBlob, 'record.webm');
    
    try {
        status.textContent = 'Traitement en cours...';
        const response = await fetch('/process_audio', {
            method: 'POST',
            body: formData
        });
        
        const data = await response.json();
        
        if (data.status === 'success') {
            result.classList.remove('hidden');
            transcription.textContent = data.transcription;
            document.getElementById('llm-response').textContent = data.llm_response; // Correction ici
            
            if (data.audio_file) {
                status.textContent = 'Lecture de la réponse...';
                await playResponse(data.audio_file);
            }
        } else {
            throw new Error(data.error || 'Erreur inconnue');
        }
    } catch (error) {
        console.error('Erreur:', error);
        status.textContent = 'Erreur lors du traitement';
    } finally {
        status.textContent = 'Prêt pour un nouvel enregistrement';
    }
};
    }

    function detectSilence() {
        analyser.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((acc, val) => acc + val, 0) / dataArray.length;
        return average < SILENCE_THRESHOLD;
    }

    async function startRecording() {
        try {
            if (!mediaRecorder) {
                await setupAudio();
            }
            
            audioChunks = [];
            mediaRecorder.start(1000);
            isRecording = true;
            status.textContent = 'Enregistrement en cours...';
            recordButton.querySelector('span').textContent = 'Arrêter';
            recordButton.style.background = 'linear-gradient(45deg, #6b7280, #4b5563)';
            
            silenceDetectionInterval = setInterval(() => {
                if (detectSilence()) {
                    if (!silenceTimer) {
                        silenceTimer = setTimeout(() => {
                            if (isRecording) {
                                stopRecording();
                            }
                        }, SILENCE_DURATION);
                    }
                } else {
                    if (silenceTimer) {
                        clearTimeout(silenceTimer);
                        silenceTimer = null;
                    }
                }
            }, 100);
            
        } catch (error) {
            console.error('Erreur:', error);
            status.textContent = 'Erreur lors de l\'accès au microphone';
        }
    }

    function stopRecording() {
        if (mediaRecorder && mediaRecorder.state === 'recording') {
            mediaRecorder.stop();
            isRecording = false;
            clearInterval(silenceDetectionInterval);
            if (silenceTimer) {
                clearTimeout(silenceTimer);
                silenceTimer = null;
            }
            recordButton.querySelector('span').textContent = 'Commencer l\'enregistrement';
            recordButton.style.background = 'linear-gradient(45deg, #ef4444, #f43f5e)';
        }
    }

    async function playResponse(audioFile) {
        try {
            const response = await fetch(`/audio/${audioFile}`);
            const blob = await response.blob();
            const url = URL.createObjectURL(blob);
            
            return new Promise((resolve) => {
                const audioPlayer = document.getElementById('audioPlayer');
                audioPlayer.src = url;
                audioPlayer.onended = () => {
                    URL.revokeObjectURL(url);
                    resolve();
                };
                audioPlayer.play();
            });
        } catch (error) {
            console.error('Erreur audio:', error);
        }
    }

    recordButton.addEventListener('click', () => {
        if (!isRecording) {
            startRecording();
        } else {
            stopRecording();
        }
    });

    window.addEventListener('beforeunload', () => {
        if (mediaRecorder) {
            stopRecording();
        }
    });
    </script>
</body>
</html>